ğŸ¤– RAG Chatbot - Intelligent Document Q&A System

A production-ready Retrieval-Augmented Generation (RAG) chatbot that enables intelligent question-answering from PDF documents using state-of-the-art AI models.

âš ï¸ Note: This is currently the backend API only. Frontend development is planned for future phases.

ğŸ“‹ Table of Contents

Overview
Key Features
Tech Stack
RAG Architecture
Project Structure
Installation
Usage
API Endpoints
How It Works
Future Enhancements


ğŸ¯ Overview
This RAG (Retrieval-Augmented Generation) chatbot allows users to upload PDF documents and ask natural language questions about their content. The system intelligently retrieves relevant information from the documents and generates accurate, context-aware answers using a large language model.
What makes this special:

ğŸ” Semantic Search: Finds information based on meaning, not just keywords
ğŸ§  AI-Powered Answers: Generates human-like responses using LLM
ğŸ“„ Source Citations: Shows exactly where information comes from (page numbers)
ğŸ’¾ Persistent Storage: Documents and embeddings stored permanently
âš¡ Fast & Scalable: Optimized for production use


âœ¨ Key Features
Core Functionality

âœ… PDF Document Upload & Processing
âœ… Intelligent Text Chunking with overlap for context preservation
âœ… Semantic Embeddings using sentence transformers
âœ… Vector Database Storage with ChromaDB
âœ… Semantic Search for relevant content retrieval
âœ… LLM-based Answer Generation with source attribution
âœ… RESTful API with interactive documentation

Technical Features

âœ… Batch processing for large documents
âœ… Automatic text cleaning and preprocessing
âœ… Persistent vector storage (no re-indexing required)
âœ… Configurable retrieval parameters (top-k, temperature, etc.)
âœ… Error handling and logging throughout
âœ… Memory-optimized for CPU inference


ğŸ› ï¸ Tech Stack
Backend Framework

FastAPI - Modern, high-performance web framework
Uvicorn - ASGI server for production deployment
Pydantic - Data validation using Python type annotations

AI/ML Stack

LangChain - Framework for LLM application development
Sentence Transformers - State-of-the-art text embeddings

Model: all-MiniLM-L6-v2 (384-dimensional embeddings)


Hugging Face Transformers - LLM infrastructure

Model: TinyLlama-1.1B-Chat-v1.0 (efficient chat model)


PyTorch - Deep learning framework

Vector Database

ChromaDB - Open-source embedding database with persistent storage

Document Processing

pypdf - PDF text extraction
RecursiveCharacterTextSplitter - Intelligent text chunking


ğŸ—ï¸ RAG Architecture
High-Level Flow
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERACTION                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DOCUMENT INGESTION PHASE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. PDF Upload                                                   â”‚
â”‚  2. Text Extraction        â†’ pypdf                               â”‚
â”‚  3. Text Chunking          â†’ LangChain (800 chars, 150 overlap) â”‚
â”‚  4. Generate Embeddings    â†’ Sentence Transformers              â”‚
â”‚  5. Store in Vector DB     â†’ ChromaDB                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     QUERY PROCESSING PHASE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. User Question                                                â”‚
â”‚  2. Generate Query Embedding                                     â”‚
â”‚  3. Similarity Search (Cosine)                                   â”‚
â”‚  4. Retrieve Top-K Chunks                                        â”‚
â”‚  5. Build Context from Retrieved Chunks                          â”‚
â”‚  6. Create Prompt: Context + Question                            â”‚
â”‚  7. LLM Generation                                               â”‚
â”‚  8. Return Answer + Sources                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Detailed RAG Pipeline
python# Step-by-step RAG workflow:

1. INDEXING (One-time per document):
   PDF â†’ Text Extraction â†’ Chunking â†’ Embeddings â†’ Vector DB

2. RETRIEVAL (Per query):
   Question â†’ Query Embedding â†’ Similarity Search â†’ Top-K Chunks

3. AUGMENTATION:
   Retrieved Chunks + Question â†’ Structured Prompt

4. GENERATION:
   Prompt â†’ LLM â†’ Answer + Source Citations
Why RAG?
Traditional LLMs have limitations:

âŒ Limited context window
âŒ No access to private/recent documents
âŒ Can hallucinate information

RAG solves this by:

âœ… Grounding answers in actual document content
âœ… Providing source citations
âœ… Scaling to unlimited documents
âœ… Always using up-to-date information


ğŸ“ Project Structure
rag-chatbot/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI application & endpoints
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py        # PDF text extraction logic
â”‚   â”‚   â”œâ”€â”€ text_chunker.py         # Text splitting & cleaning
â”‚   â”‚   â”œâ”€â”€ embeddings_service.py   # Embedding generation
â”‚   â”‚   â”œâ”€â”€ vector_store.py         # ChromaDB operations
â”‚   â”‚   â”œâ”€â”€ llm_service.py          # LLM inference & prompt handling
â”‚   â”‚   â””â”€â”€ rag_pipeline.py         # Complete RAG workflow
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ uploads/                # Uploaded PDF storage
â”‚   â”‚   â””â”€â”€ chroma_db/              # Vector database persistence
â”‚   â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚   â”œâ”€â”€ .env                        # Environment variables
â”‚   â””â”€â”€ preload_documents.py        # Batch PDF processing script
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ğŸš€ Installation
Prerequisites

Python 3.10 or higher
8GB+ RAM (16GB recommended for optimal performance)
5GB free disk space (for models and data)

Setup Instructions

Clone the repository

bashgit clone <repository-url>
cd rag-chatbot

Create virtual environment

bashcd backend
python -m venv venv

# Windows
venv\Scripts\activate

# Mac/Linux
source venv/bin/activate

Install dependencies

bashpip install -r requirements.txt

Create data directories

bashmkdir -p data/uploads data/chroma_db

Start the server

bashuvicorn app.main:app --reload --host 0.0.0.0 --port 8000
